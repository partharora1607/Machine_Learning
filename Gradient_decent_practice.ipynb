import numpy as np


def apply_gd(data, learning_rate, m, c):
    m_slope = 0
    c_slope = 0
    M = len(data)
    for i in range(M):
        x = data[i, 0]
        y = data[i, 1]
        m_slope += (-2/M) * (y - m*x - c) * x
        c_slope += (-2/M) * (y - m*x - c)
    m_new = m - learning_rate * m_slope
    c_new = c - learning_rate * c_slope
    return m_new,c_new 

def cost_function(data , m , c):
    initial_cost = 0
    

def gd(data, learning_rate, iterations):
    # step - 1 , initialize m , c with any random value let say 0 , 0
    m = 0
    c = 0
    for i in range(iterations):
        m, c = apply_gd(data, learning_rate, m, c)
        print(i , " cost: ", cost_function(data , m , c))
    return m, c


def apply_gradient_decent_to_minimize_cost_function():
    data = np.loadtxt("data.csv", delimiter=',')
    learning_rate = 0.0001
    iterations = 100
    m, c = gd(data, learning_rate, iterations)
    print(m, c)

def cost_function(data , m , c):
    cost = 0
    for i in range(len(data)):
        x = data[i,0]
        y = data[i,1]
        cost += (1/len(data)) * ((y - m*x - c) ** 2)
    return cost
